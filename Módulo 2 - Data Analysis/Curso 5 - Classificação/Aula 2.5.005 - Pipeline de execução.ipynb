{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Classificadores Lineares\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Classificadores KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Classificadores Naive Nayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Classificadores Arvores de Decisão\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, classification_report\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn import linear_model\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "from scipy.io import arff\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, LeaveOneOut, ShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "# Pipeline\n",
    "* Definir um workflow de análise\n",
    "* Criar um workflow que conecta transformadores e estimadores em uma sequência.\n",
    "* Cada etapa é realizada por uma classe, que são encapsuladas em uma classe maior e executadas sob controle de um pipeline \n",
    "\n",
    "### Salvando e Carregando modelos para arquivos\n",
    "* joblib \n",
    "\n",
    "### Busca por hiperparametros\n",
    "* Grid Search\n",
    "* Random Search \n",
    "\n",
    "### Validação Cruzada\n",
    "* Kfolds\n",
    "* Kfolds estratificados\n",
    "* LOOCV\n",
    "repetição de divisão entre treino e teste \n",
    "\n",
    "### Escolha de características (Features)\n",
    "* kbest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No código abaixo está sendo feita a comparação entre 3 modelos de classificação\n",
    "Preparação dos dados para classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/SAheart.csv')\n",
    "\n",
    "X = df.drop(['chd'], axis=1)\n",
    "y = df['chd']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "X['famhist'] = le.fit_transform(X['famhist'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7956989247311828\n",
      "0.7311827956989247\n",
      "0.6344086021505376\n",
      "0.6236559139784946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lazaropd\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Comparação do score dos modelos\n",
    "\n",
    "sgd = LogisticRegression(random_state=42)\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test)\n",
    "print(sgd.score(X_test, y_test))\n",
    "\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test)\n",
    "print(sgd.score(X_test, y_test))\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(tree.score(X_test, y_test))\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Código complexo e dificil de manter\n",
    "* Pipelines auxiliam sistematizar a avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo um workflow com pipeline\n",
    "* A mesma análise anterior, porem, incluindo cada avaliação em um objeto pipe\n",
    "* Ao final um pipeline com outros pipes sao executados\n",
    "* Um pipe possui uma ou mais transformacoes,por exemplo, normalização, padronizaçõ, etc (que possue a funcao transform), e por último um modelo de predição (que possui a função fit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression pipeline test accuracy: 0.828\n",
      "KNN pipeline test accuracy: 0.645\n",
      "Decision Tree pipeline test accuracy: 0.656\n",
      "Classifier with best accuracy: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Construindo pipelines\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression())])\n",
    "pipe_knn = Pipeline([('scl', StandardScaler()), ('clf', KNeighborsClassifier())])\n",
    "pipe_dt = Pipeline([('scl', StandardScaler()), ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "# Lista de pipelines a serem executados\n",
    "pipelines = [pipe_lr, pipe_knn, pipe_dt]\n",
    "\n",
    "# Dicionário para facilitar identificacao\n",
    "pipe_dict = {0: 'Logistic Regression', 1: 'KNN', 2: 'Decision Tree'}\n",
    "\n",
    "# aplicando fit\n",
    "# Generaliza a execucao do fit de cada ultima funcao do pipe\n",
    "for pipe in pipelines:\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "# Compara acurácia\n",
    "for idx, val in enumerate(pipelines):\n",
    "    print('%s pipeline test accuracy: %.3f' % (pipe_dict[idx], val.score(X_test, y_test)))\n",
    "\n",
    "# para cada modelo treinado obtem val score\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_pipe = ''\n",
    "for idx, val in enumerate(pipelines):\n",
    "    # Descobre o melhor val.score e armazen em best_clf\n",
    "    if val.score(X_test, y_test) > best_acc:\n",
    "        best_acc = val.score(X_test, y_test)\n",
    "        best_pipe = val\n",
    "        best_clf = idx\n",
    "print('Classifier with best accuracy: %s' % pipe_dict[best_clf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando um modelo para um arquivo usando joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pipeline to file\n",
    "joblib.dump(best_pipe, 'best_pipeline.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando um modelo salvo em arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_model = joblib.load('best_pipeline.pkl')\n",
    "\n",
    "y_pred = joblib_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busca por hiperparametros\n",
    "* Hiperparametros sao as diversas parametrizações possíveis de um modelo de predição\n",
    "* Usando gridsearchCV: busca todas as combinações definidas de hiperparametros, e retorna combinação com o melhor score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'alpha': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.693 (+/-0.067) for {'alpha': 1}\n",
      "0.689 (+/-0.071) for {'alpha': 10}\n",
      "0.673 (+/-0.065) for {'alpha': 100}\n",
      "0.664 (+/-0.095) for {'alpha': 1000}\n",
      "\n",
      "classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86        62\n",
      "           1       0.74      0.65      0.69        31\n",
      "\n",
      "    accuracy                           0.81        93\n",
      "   macro avg       0.79      0.77      0.77        93\n",
      "weighted avg       0.80      0.81      0.80        93\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'alpha': 1}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.665 (+/-0.055) for {'alpha': 1}\n",
      "0.663 (+/-0.062) for {'alpha': 10}\n",
      "0.640 (+/-0.054) for {'alpha': 100}\n",
      "0.619 (+/-0.050) for {'alpha': 1000}\n",
      "\n",
      "classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86        62\n",
      "           1       0.74      0.65      0.69        31\n",
      "\n",
      "    accuracy                           0.81        93\n",
      "   macro avg       0.79      0.77      0.77        93\n",
      "weighted avg       0.80      0.81      0.80        93\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'alpha': [1, 10, 100, 1000]},]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    \n",
    "    clf = GridSearchCV(\n",
    "        RidgeClassifier(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"classification report:\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando RandomizedSearchCV: busca um conjunto aleatório de combinações possíveis de hiperparametros, e retorna o modelo com o melhor score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lazaropd\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\sklearn\\model_selection\\_search.py:281: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200, random_state=0)\n",
    "distributions = dict(penalty=['l2', 'l1'])\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "search = clf.fit(X_test, y_test)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Cruzada\n",
    "#### K-fold\n",
    "fold: sub conjunto de dados para teste e treino\n",
    "Sub conjuntos gerados movimentando um índice nos dados para definir o início e fim dos dados de treino\n",
    "número K define quantidade de movimentações para gerar base de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 7] [0 6]\n",
      "[0.2 0.3 0.4 0.5 0.6 0.8] [0.1 0.7]\n",
      "[0 1 3 5 6 7] [2 4]\n",
      "[0.1 0.2 0.4 0.6 0.7 0.8] [0.3 0.5]\n",
      "[0 1 2 3 4 6] [5 7]\n",
      "[0.1 0.2 0.3 0.4 0.5 0.7] [0.6 0.8]\n",
      "[0 2 4 5 6 7] [1 3]\n",
      "[0.1 0.3 0.5 0.6 0.7 0.8] [0.2 0.4]\n"
     ]
    }
   ],
   "source": [
    "# Amostra de dados\n",
    "data = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n",
    "# Montando Folds\n",
    "kfold = KFold(4, shuffle=True)\n",
    "\n",
    "\n",
    "for train, test in kfold.split(data):\n",
    "    print(train , test)\n",
    "    print(data[train] , data[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sbp  tobacco   ldl  adiposity  typea  obesity  alcohol  age  chd  \\\n",
      "1  160    12.00  5.73      23.11     49    25.30    97.20   52    1   \n",
      "2  144     0.01  4.41      28.61     55    28.87     2.06   63    1   \n",
      "3  118     0.08  3.48      32.28     52    29.14     3.81   46    0   \n",
      "4  170     7.50  6.41      38.03     51    31.99    24.26   58    1   \n",
      "5  134    13.60  3.50      27.78     60    25.99    57.34   49    1   \n",
      "\n",
      "   famhist_label  \n",
      "1              1  \n",
      "2              0  \n",
      "3              1  \n",
      "4              1  \n",
      "5              1  \n"
     ]
    }
   ],
   "source": [
    "heart = pd.read_csv('dataset/SAheart.csv')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "heart['famhist_label'] = le.fit_transform(heart['famhist'])\n",
    "heart.drop('famhist', axis=1, inplace=True)\n",
    "\n",
    "print(heart.head())\n",
    "\n",
    "X = heart[['sbp','tobacco','ldl','adiposity','typea','obesity','alcohol','age','famhist_label']] \n",
    "y = heart['chd']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia: 53.19%\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(random_state=42)\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test)\n",
    "print(\"Acuracia: %.2f%%\" % (sgd.score(X_test, y_test)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste com Kfolds\n",
    "* Montando 10 folds para o mesmo classificador testado acima\n",
    "* cross_val_score aplica os folds e obtem o score do fit de cada sub conjunto  \n",
    "Ao final mostra a média das acurácias obtidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.57142857 0.61904762 0.64285714 0.47619048 0.66666667 0.53658537\n",
      " 0.85365854 0.68292683 0.6097561  0.68292683]\n",
      "Acuracia: 63.42%\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "model_kfold = SGDClassifier()\n",
    "results_kfold = cross_val_score(model_kfold, X_train, y_train, cv=kfold)\n",
    "print(\"scores: \", results_kfold) \n",
    "print(\"Acuracia: %.2f%%\" % (results_kfold.mean()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folds Stratificados\n",
    "Utiliza folds estratificados: cada conjunto contendo aproximadamente a mesma proporção de labels de destino que os dados completos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.68571429 0.68115942 0.66666667 0.52173913 0.65217391 0.63768116]\n",
      "Accuracy: 64.09%\n"
     ]
    }
   ],
   "source": [
    "skfold = StratifiedKFold(n_splits=6, shuffle=True, random_state=100)\n",
    "model_skfold = SGDClassifier()\n",
    "results_skfold = cross_val_score(model_skfold, X_train, y_train, cv=skfold)\n",
    "print(\"scores: \", results_skfold) \n",
    "print(\"Accuracy: %.2f%%\" % (results_skfold.mean()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out Cross-Validation (LOOCV)\n",
    "* Os Fold são definidos com tamanho 1 e K o número de observações\n",
    "* Essa variação é útil quando os dados de treinamento são de tamanho limitado e o número de parâmetros a serem testados não é alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.41%\n"
     ]
    }
   ],
   "source": [
    "loocv = LeaveOneOut()\n",
    "model_loocv = SGDClassifier()\n",
    "results_loocv = cross_val_score(model_loocv, X_train, y_train, cv=loocv)\n",
    "#print(\"scores: \", results_loocv) \n",
    "print(\"Accuracy: %.2f%%\" % (results_loocv.mean()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repetição Aleatória de divisão entre treino e teste (Repeated Random Test-Train Splits)\n",
    "* Híbrido entre divisão tradicional de teste de trem e do método de validação cruzada de k Fold.\n",
    "* Nesta técnica, divisões entre treino e teste aleatórias são criadas nos dados da maneira definida pelo conjunto de testes de treinamento\n",
    "* Esse processo é repetidos várias vezes, assim como o método de validação cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.68  0.632 0.696 0.328 0.64  0.656 0.568 0.736 0.52  0.712]\n",
      "Accuracy: 61.68% (11.45%)\n"
     ]
    }
   ],
   "source": [
    "kfold2 = ShuffleSplit(n_splits=10, test_size=0.30, random_state=100)\n",
    "model_shufflecv = SGDClassifier()\n",
    "results_4 = cross_val_score(model_shufflecv, X_train, y_train, cv=kfold2)\n",
    "print(\"scores: \", results_4) \n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results_4.mean()*100.0, results_4.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.735\n",
      "\n",
      "Best params:\n",
      " {'clf__criterion': 'gini', 'clf__max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "#pipe_knn = Pipeline([('scl', StandardScaler()), ('clf', KNeighborsClassifier())])\n",
    "pipetree = Pipeline([('scl', StandardScaler()), ('clf', DecisionTreeClassifier())])\n",
    "\n",
    "#pipe = [pipe_knn, pipetre]\n",
    "pipe = [pipetree]\n",
    "\n",
    "param_range = [1, 2, 3, 4, 5]\n",
    "\n",
    "# grid search params\n",
    "grid_params = [{'clf__criterion': ['gini', 'entropy'],\n",
    "               'clf__max_depth': param_range}]\n",
    "#grid_params = [{'clf__criterion': ['gini', 'entropy'],\n",
    "#    'clf__min_samples_leaf': param_range,\n",
    "#    'clf__max_depth': param_range,\n",
    "#    'clf__min_samples_split': param_range[1:],\n",
    "#    'clf__presort': [True, False]}]\n",
    "\n",
    "# Construct grid search\n",
    "gs = GridSearchCV(estimator=pipetree,\n",
    "    param_grid=grid_params,\n",
    "    scoring='accuracy')\n",
    "\n",
    "# Fit using grid search\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Best accuracy\n",
    "print('Best accuracy: %.3f' % gs.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolhendo Features\n",
    "* Método kbest\n",
    "* Utiliza método chi quadrado para escolher melhor conjunto de features de acordo com o seguinte critério:\n",
    "* Realiza um teste estatístico usando qui-quadrado entre cada features e classe\n",
    "* O teste do qui-quadrado “elimina” features com maior probabilidade de serem independentes da classe e, portanto, irrelevantes para a classificação.  \n",
    "Fonte: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sbp  tobacco   ldl  adiposity  typea  obesity  alcohol  age  chd  \\\n",
      "1  160    12.00  5.73      23.11     49    25.30    97.20   52    1   \n",
      "2  144     0.01  4.41      28.61     55    28.87     2.06   63    1   \n",
      "3  118     0.08  3.48      32.28     52    29.14     3.81   46    0   \n",
      "4  170     7.50  6.41      38.03     51    31.99    24.26   58    1   \n",
      "5  134    13.60  3.50      27.78     60    25.99    57.34   49    1   \n",
      "\n",
      "   famhist_label  \n",
      "1              1  \n",
      "2              0  \n",
      "3              1  \n",
      "4              1  \n",
      "5              1  \n",
      "(462, 9)\n",
      "(462, 3)\n"
     ]
    }
   ],
   "source": [
    "heart = pd.read_csv('dataset/SAheart.csv')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "heart['famhist_label'] = le.fit_transform(heart['famhist'])\n",
    "heart.drop('famhist', axis=1, inplace=True)\n",
    "\n",
    "print(heart.head())\n",
    "\n",
    "X = heart[['sbp','tobacco','ldl','adiposity','typea','obesity','alcohol','age','famhist_label']] \n",
    "y = heart['chd'] \n",
    "\n",
    "print(X.shape)\n",
    "X_new = SelectKBest(chi2, k=3).fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
