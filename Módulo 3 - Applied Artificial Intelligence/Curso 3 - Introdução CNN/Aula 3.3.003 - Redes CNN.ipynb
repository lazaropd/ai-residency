{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications import Xception # TensorFlow ONLY\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import image\n",
    "from tensorflow.python.keras.applications.inception_v3 import *\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montando uma Rede Neural Convolucional (CNN)\n",
    "* Convolução\n",
    "    * conv2d\n",
    "* Pooling\n",
    "    * MaxPooling2D    \n",
    "    * AveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando a Rede Neural Com Dados da Base Mnist (Dígitos\n",
    "* Dividindo os valores por 255 para normalização (transforma para 0~1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalmente, redes CNNs são montadas da seguinte forma:\n",
    "* Sequência de camadas de convolução e pooling\n",
    "* Camada flatten\n",
    "    * Realiza uma operação de transformar um vetor com múltiplas dimensões para um vetor com apenas uma dimensão. Necessário para adequar corretamente o resultado da convolução com a camada de classificação\n",
    "* Camada densa com ativação softmax para classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Após o processo de Convolução é utilizada uma camada Dense para classificar entre as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                54090     \n",
      "=================================================================\n",
      "Total params: 54,410\n",
      "Trainable params: 54,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 144us/sample - loss: 0.2132 - accuracy: 0.9390\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0859 - accuracy: 0.9757\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 139us/sample - loss: 0.0692 - accuracy: 0.9795\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0603 - accuracy: 0.9829\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0559 - accuracy: 0.9845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f816617b790>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5)#, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 103us/sample - loss: 0.0579 - accuracy: 0.9811\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9821"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando rede Neural Treinada para realizar Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_img('figs/mug.jpg', target_size=(224, 224))\n",
    "#image = load_img('figs/mug.jpg', target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. toilet_tissue: 10.89%\n",
      "2. mosquito_net: 7.59%\n",
      "3. shower_curtain: 5.20%\n",
      "4. toilet_seat: 3.65%\n",
      "5. paper_towel: 1.75%\n"
     ]
    }
   ],
   "source": [
    "# Converter pixels em um array\n",
    "image = img_to_array(image)\n",
    "\n",
    "# acertar formato do array para o modelo\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "# preprocessamento da imagem\n",
    "image = preprocess_input(image)\n",
    "\n",
    "# calcular probabilidade considerando todos os \"labels\"\n",
    "yhat = model.predict(image)\n",
    "\n",
    "# Codificar as probabilidades retornadas nos \"labels\"\n",
    "label = decode_predictions(yhat)\n",
    "\n",
    "for (i, (imagenetID, label, prob)) in enumerate(label[0]):\n",
    "\tprint(\"{}. {}: {:.2f}%\".format(i + 1, label, prob * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figs/mug.jpg\r\n"
     ]
    }
   ],
   "source": [
    "!ls figs/mug.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. coffee_mug: 66.749%\n",
      "2. cup: 12.973%\n",
      "3. teapot: 0.715%\n",
      "4. coffeepot: 0.699%\n",
      "5. pitcher: 0.490%\n"
     ]
    }
   ],
   "source": [
    "model = InceptionV3(weights='imagenet')\n",
    "\n",
    "img = image.load_img('figs/mug.jpg', target_size=(299, 299))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "y = model.predict(x)\n",
    "for index, res in enumerate(decode_predictions(y)[0]):\n",
    "    print('{}. {}: {:.3f}%'.format(index + 1, res[1], 100 * res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando MLP x CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21000, 80, 80)\n",
      "(21000,)\n",
      "[[[[241]\n",
      "   [236]\n",
      "   [235]\n",
      "   ...\n",
      "   [252]\n",
      "   [252]\n",
      "   [252]]\n",
      "\n",
      "  [[245]\n",
      "   [240]\n",
      "   [242]\n",
      "   ...\n",
      "   [252]\n",
      "   [252]\n",
      "   [252]]\n",
      "\n",
      "  [[253]\n",
      "   [250]\n",
      "   [250]\n",
      "   ...\n",
      "   [252]\n",
      "   [252]\n",
      "   [252]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[167]\n",
      "   [166]\n",
      "   [177]\n",
      "   ...\n",
      "   [252]\n",
      "   [252]\n",
      "   [252]]\n",
      "\n",
      "  [[149]\n",
      "   [154]\n",
      "   [142]\n",
      "   ...\n",
      "   [252]\n",
      "   [252]\n",
      "   [252]]\n",
      "\n",
      "  [[111]\n",
      "   [139]\n",
      "   [120]\n",
      "   ...\n",
      "   [252]\n",
      "   [252]\n",
      "   [252]]]\n",
      "\n",
      "\n",
      " [[[152]\n",
      "   [ 66]\n",
      "   [109]\n",
      "   ...\n",
      "   [232]\n",
      "   [206]\n",
      "   [133]]\n",
      "\n",
      "  [[102]\n",
      "   [146]\n",
      "   [ 96]\n",
      "   ...\n",
      "   [159]\n",
      "   [138]\n",
      "   [202]]\n",
      "\n",
      "  [[114]\n",
      "   [ 91]\n",
      "   [ 66]\n",
      "   ...\n",
      "   [175]\n",
      "   [122]\n",
      "   [120]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[163]\n",
      "   [173]\n",
      "   [195]\n",
      "   ...\n",
      "   [150]\n",
      "   [156]\n",
      "   [224]]\n",
      "\n",
      "  [[181]\n",
      "   [165]\n",
      "   [171]\n",
      "   ...\n",
      "   [135]\n",
      "   [225]\n",
      "   [170]]\n",
      "\n",
      "  [[223]\n",
      "   [155]\n",
      "   [148]\n",
      "   ...\n",
      "   [163]\n",
      "   [166]\n",
      "   [191]]]\n",
      "\n",
      "\n",
      " [[[172]\n",
      "   [156]\n",
      "   [153]\n",
      "   ...\n",
      "   [128]\n",
      "   [107]\n",
      "   [ 96]]\n",
      "\n",
      "  [[176]\n",
      "   [162]\n",
      "   [159]\n",
      "   ...\n",
      "   [108]\n",
      "   [103]\n",
      "   [108]]\n",
      "\n",
      "  [[165]\n",
      "   [167]\n",
      "   [156]\n",
      "   ...\n",
      "   [ 72]\n",
      "   [ 57]\n",
      "   [ 84]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[144]\n",
      "   [135]\n",
      "   [121]\n",
      "   ...\n",
      "   [132]\n",
      "   [149]\n",
      "   [144]]\n",
      "\n",
      "  [[141]\n",
      "   [135]\n",
      "   [122]\n",
      "   ...\n",
      "   [161]\n",
      "   [147]\n",
      "   [134]]\n",
      "\n",
      "  [[141]\n",
      "   [136]\n",
      "   [122]\n",
      "   ...\n",
      "   [148]\n",
      "   [142]\n",
      "   [151]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[203]\n",
      "   [175]\n",
      "   [223]\n",
      "   ...\n",
      "   [170]\n",
      "   [172]\n",
      "   [175]]\n",
      "\n",
      "  [[220]\n",
      "   [125]\n",
      "   [159]\n",
      "   ...\n",
      "   [162]\n",
      "   [167]\n",
      "   [173]]\n",
      "\n",
      "  [[205]\n",
      "   [234]\n",
      "   [221]\n",
      "   ...\n",
      "   [165]\n",
      "   [166]\n",
      "   [180]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[140]\n",
      "   [137]\n",
      "   [138]\n",
      "   ...\n",
      "   [119]\n",
      "   [123]\n",
      "   [120]]\n",
      "\n",
      "  [[135]\n",
      "   [138]\n",
      "   [135]\n",
      "   ...\n",
      "   [122]\n",
      "   [119]\n",
      "   [119]]\n",
      "\n",
      "  [[135]\n",
      "   [135]\n",
      "   [135]\n",
      "   ...\n",
      "   [123]\n",
      "   [123]\n",
      "   [118]]]\n",
      "\n",
      "\n",
      " [[[ 28]\n",
      "   [ 33]\n",
      "   [ 23]\n",
      "   ...\n",
      "   [ 26]\n",
      "   [ 27]\n",
      "   [ 25]]\n",
      "\n",
      "  [[ 30]\n",
      "   [ 32]\n",
      "   [ 35]\n",
      "   ...\n",
      "   [ 26]\n",
      "   [ 27]\n",
      "   [ 25]]\n",
      "\n",
      "  [[ 36]\n",
      "   [ 23]\n",
      "   [ 21]\n",
      "   ...\n",
      "   [ 28]\n",
      "   [ 26]\n",
      "   [ 25]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 10]\n",
      "   [  9]\n",
      "   [  9]\n",
      "   ...\n",
      "   [ 11]\n",
      "   [ 11]\n",
      "   [ 10]]\n",
      "\n",
      "  [[ 14]\n",
      "   [ 14]\n",
      "   [ 15]\n",
      "   ...\n",
      "   [ 10]\n",
      "   [ 11]\n",
      "   [ 11]]\n",
      "\n",
      "  [[ 13]\n",
      "   [  8]\n",
      "   [ 11]\n",
      "   ...\n",
      "   [ 11]\n",
      "   [ 13]\n",
      "   [ 13]]]\n",
      "\n",
      "\n",
      " [[[175]\n",
      "   [175]\n",
      "   [118]\n",
      "   ...\n",
      "   [134]\n",
      "   [191]\n",
      "   [198]]\n",
      "\n",
      "  [[120]\n",
      "   [131]\n",
      "   [138]\n",
      "   ...\n",
      "   [140]\n",
      "   [189]\n",
      "   [197]]\n",
      "\n",
      "  [[ 91]\n",
      "   [115]\n",
      "   [142]\n",
      "   ...\n",
      "   [154]\n",
      "   [182]\n",
      "   [198]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[163]\n",
      "   [146]\n",
      "   [115]\n",
      "   ...\n",
      "   [135]\n",
      "   [117]\n",
      "   [109]]\n",
      "\n",
      "  [[117]\n",
      "   [168]\n",
      "   [158]\n",
      "   ...\n",
      "   [131]\n",
      "   [ 96]\n",
      "   [138]]\n",
      "\n",
      "  [[171]\n",
      "   [128]\n",
      "   [126]\n",
      "   ...\n",
      "   [131]\n",
      "   [154]\n",
      "   [145]]]]\n",
      "[1 1 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "main_dir=\"/home/silvio/git/datasets/dogsCatsDB/\"\n",
    "train_dir = \"train\"\n",
    "path = os.path.join(main_dir,train_dir)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "convert = lambda category : int(category == 'dog')\n",
    "\n",
    "# Percorre os arquivos no diretório de imagens para treinamento\n",
    "\n",
    "def create_test_data(path):\n",
    "    for p in os.listdir(path):\n",
    "\n",
    "        # categoria da imagem é definida pelo nome do arquivo\n",
    "        category = p.split(\".\")[0]\n",
    "\n",
    "        # define categoria como 0 ou 1\n",
    "        category = convert(category)\n",
    "        \n",
    "        # Abre a imagem usando opencv em escala de cinza\n",
    "        img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Redimensionamento para 80 x 80 pixels\n",
    "        new_img_array = cv2.resize(img_array, dsize=(80, 80))\n",
    "        \n",
    "        X.append(new_img_array)\n",
    "        y.append(category)\n",
    "\n",
    "path=\"/home/silvio/git/datasets/dogsCatsDB/train\"\n",
    "\n",
    "create_test_data(path)\n",
    "\n",
    "print(np.array(X).shape)\n",
    "print(np.array(y).shape)\n",
    "X = np.array(X).reshape(-1, 80,80,1)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21000, 80, 80, 1)\n",
      "(21000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "* conv2d\n",
    "* maxpooling2d\n",
    "* padding\n",
    "    * same = aplica valores 0 ao redor de toda imagem, mantendo tamanho original da matriz após aplicar filtros   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 5s 257us/sample - loss: 18.0992 - accuracy: 0.5786\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 3s 167us/sample - loss: 0.6419 - accuracy: 0.6414\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 3s 165us/sample - loss: 0.5901 - accuracy: 0.6840\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 4s 168us/sample - loss: 0.5527 - accuracy: 0.7105\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 3s 167us/sample - loss: 0.5142 - accuracy: 0.7346\n",
      "21000/21000 [==============================] - 2s 110us/sample - loss: 0.4580 - accuracy: 0.7657\n",
      "0.7657143\n"
     ]
    }
   ],
   "source": [
    "y_binary = to_categorical(y)\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(80, 80, 1), padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y_binary, epochs=5)#, batch_size=64)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X, y_binary)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmlp = np.array(X).reshape(21000,6400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 3s 162us/sample - loss: 29.1505 - accuracy: 0.5045\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 3s 152us/sample - loss: 1.1081 - accuracy: 0.5009\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 3s 152us/sample - loss: 1.0036 - accuracy: 0.4969\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 3s 151us/sample - loss: 1.0490 - accuracy: 0.4949\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 3s 152us/sample - loss: 1.0188 - accuracy: 0.4966\n",
      "21000/21000 [==============================] - 2s 95us/sample - loss: 0.6931 - accuracy: 0.5001\n",
      "0.50009525\n"
     ]
    }
   ],
   "source": [
    "y_binary = to_categorical(y)\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(6400,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(Xmlp, y_binary, epochs=5)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(Xmlp, y_binary)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
